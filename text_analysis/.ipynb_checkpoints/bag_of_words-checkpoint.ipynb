{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFoXOLWRPw_p"
   },
   "source": [
    "# Turning Text into Numbers\n",
    "\n",
    "Learning Objectives:\n",
    "- Introduce bag-of-words representation for text\n",
    "- Apply bag-of-words approach to sentiment analysis task\n",
    "\n",
    "One of the key benefits of data wrangling and analysis is pulling out patterns from complex data. One rich source of data is text, including social media posts to novels to news corpora.\n",
    "\n",
    "The key question surrounding text data is: how do we go from words to numbers that are compatible with models and statistical analysis? Today we will go through an example using movie review data and sentiment analysis for text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDdQG12Z64AP"
   },
   "source": [
    "## Movie Review Dataset and Sentiment Analysis\n",
    "\n",
    "\n",
    "Today we'll use a [data set](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download) which contains reviews of movies from IMDB. These reviews reflect someone's **sentiment**, or attitude, towards the movie they are reviewing, be it positive or negative. \n",
    "\n",
    "Let's load in the dataset and take a look at a few examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1669672513648,
     "user": {
      "displayName": "Emily Grabowski",
      "userId": "07033070354571424802"
     },
     "user_tz": 480
    },
    "id": "n7LWSNC-PslN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/emilygrabowski/demos-tutorials/main/text_analysis/IMDB_reviews_cleaned.csv\")[:10000]\n",
    "text=df['text'].values\n",
    "sentiment = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iql7Aal96FEB"
   },
   "source": [
    "**Question:** Are the reviews below negative or positive? What specific words or phrases from the text helped you to make that judgment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1669670656833,
     "user": {
      "displayName": "Emily Grabowski",
      "userId": "07033070354571424802"
     },
     "user_tz": 480
    },
    "id": "erWpHxkLjBzp",
    "outputId": "066bfdeb-c0cd-4adf-fc5f-4a299906ea6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This a fantastic movie of three prisoners who become famous. One of the actors is george clooney and I'm not a fan but this roll is not bad. Another good thing about the movie is the soundtrack (The man of constant sorrow). I recommand this movie to everybody. Greetings Bart\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example 1:\")\n",
    "text[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1669670658549,
     "user": {
      "displayName": "Emily Grabowski",
      "userId": "07033070354571424802"
     },
     "user_tz": 480
    },
    "id": "WfKfZ3A-lZaF",
    "outputId": "5603191f-2ef1-4409-8a82-76ea6544d18c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"An awful film! It must have been up against some real stinkers to be nominated for the Golden Globe. They've taken the story of the first famous female Renaissance painter and mangled it beyond recognition. My complaint is not that they've taken liberties with the facts; if the story were good, that would perfectly fine. But it's simply bizarre -- by all accounts the true story of this artist would have made for a far better film, so why did they come up with this dishwater-dull script? I suppose there weren't enough naked people in the factual version. It's hurriedly capped off in the end with a summary of the artist's life -- we could have saved ourselves a couple of hours if they'd favored the rest of the film with same brevity.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Example 2:\")\n",
    "text[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1669670659859,
     "user": {
      "displayName": "Emily Grabowski",
      "userId": "07033070354571424802"
     },
     "user_tz": 480
    },
    "id": "RBVUIn5hlc0y",
    "outputId": "b00f8e83-1790-43bf-9413-f308032f83d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can't believe it's still on the air.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Example 3:')\n",
    "text[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dt9j9D1n-KuT"
   },
   "source": [
    "## Bag-of-words\n",
    "\n",
    "\n",
    "Now that we've taken a look at the dataset, let's talk about transforming the text to numbers. In this case, we need a framework in which to assign meaningful numeric values to each review. One way to simplify this problem is to treat the text as a collection of unordered words, which alleviates the need to account for things like word order.\n",
    "\n",
    "For example, in judging the sentiment of each of those reviews, there are certain keywords and phrases that are helpful for identifying sentiment. \n",
    "\n",
    "1. Positive sentiment: Good, happy, best, awesome\n",
    "2. Negative sentiment: Worst, terrible, horrible, bad\n",
    "\n",
    "The idea that individual words are informative as to the content of a text is the logic underpinning the bag-of-words approach. You take the words in a review, put it in a bag, shake it up, and count the occurence of each word in the review. And now we have numbers!\n",
    "\n",
    "![link text](https://dudeperf3ct.github.io/images/lstm_and_gru/bag-of-words.png)\n",
    "\n",
    "\n",
    "[Image source](https://dudeperf3ct.github.io/images/lstm_and_gru/)\n",
    "\n",
    "\n",
    "**Question:** What are some cases where a bag-of-words approach might not work? (i.e. where context changes the meaning of words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1U31dfdHOpw"
   },
   "source": [
    "## Bag-of-words in Python\n",
    "\n",
    "The steps to actually generating this result is as follows:\n",
    "1. Preprocess the text (lower case, remove punctuation and formatting)\n",
    "2. Identify all of the unique words in the text\n",
    "3. Count the occurrence of each unique word in each review (which can be 0)\n",
    "4. Make a table of reviews x vocabulary \n",
    "\n",
    "\n",
    "This is all possible to do in base Python, but the more efficient and common way is to use a package developed for this purpose which will roll all of these steps together. \n",
    "\n",
    "Today, we will use a function called [CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which will *count* up all of the occurences of each word in the text and make a *vector*. We won't get into the details of the whole package that this comes from (sklearn, which is an excellent machine learning package) today, but I'll walk through the example code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1669673654990,
     "user": {
      "displayName": "Emily Grabowski",
      "userId": "07033070354571424802"
     },
     "user_tz": 480
    },
    "id": "f4Hiu04T1TUQ",
    "outputId": "4e8501c3-a748-413b-cdb5-c2aedf304c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the vocabulary: 52640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00001</th>\n",
       "      <th>0069</th>\n",
       "      <th>007</th>\n",
       "      <th>00am</th>\n",
       "      <th>00s</th>\n",
       "      <th>01</th>\n",
       "      <th>0126</th>\n",
       "      <th>01pm</th>\n",
       "      <th>...</th>\n",
       "      <th>être</th>\n",
       "      <th>ís</th>\n",
       "      <th>ísnt</th>\n",
       "      <th>île</th>\n",
       "      <th>ïn</th>\n",
       "      <th>óli</th>\n",
       "      <th>önsjön</th>\n",
       "      <th>über</th>\n",
       "      <th>überwoman</th>\n",
       "      <th>ünfaithful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 52640 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  00001  0069  007  00am  00s  01  0126  01pm  ...  être  ís  \\\n",
       "0      0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "1      0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "2      0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "3      0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "4      0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "...   ..  ...    ...   ...  ...   ...  ...  ..   ...   ...  ...   ...  ..   \n",
       "9995   0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "9996   0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "9997   0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "9998   0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "9999   0    0      0     0    0     0    0   0     0     0  ...     0   0   \n",
       "\n",
       "      ísnt  île  ïn  óli  önsjön  über  überwoman  ünfaithful  \n",
       "0        0    0   0    0       0     0          0           0  \n",
       "1        0    0   0    0       0     0          0           0  \n",
       "2        0    0   0    0       0     0          0           0  \n",
       "3        0    0   0    0       0     0          0           0  \n",
       "4        0    0   0    0       0     0          0           0  \n",
       "...    ...  ...  ..  ...     ...   ...        ...         ...  \n",
       "9995     0    0   0    0       0     0          0           0  \n",
       "9996     0    0   0    0       0     0          0           0  \n",
       "9997     0    0   0    0       0     0          0           0  \n",
       "9998     0    0   0    0       0     0          0           0  \n",
       "9999     0    0   0    0       0     0          0           0  \n",
       "\n",
       "[10000 rows x 52640 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer  = CountVectorizer() #Load the bag of words model\n",
    "X = vectorizer.fit_transform(text).toarray() # Make the bag of words representation and transform it into a matrix\n",
    "\n",
    "\n",
    "print(\"Number of items in the vocabulary:\",X.shape[1])\n",
    "dtm = pd.DataFrame(X,columns = vectorizer.get_feature_names_out()) #Add vocabulary labels to the matrix\n",
    "dtm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw3JlhtnOGX2"
   },
   "source": [
    "Now, let's answer some questions about the table: \n",
    "1. How many reviews are in the table?\n",
    "2. How many unique words are in the table?\n",
    "3. What does each of the numbers stand for? What does a row correspond to?\n",
    "\n",
    "Now let's take a look at the most common words in the data. Again, we won't worry too much about the code and will focus on the output of the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1669673935209,
     "user": {
      "displayName": "Emily Grabowski",
      "userId": "07033070354571424802"
     },
     "user_tz": 480
    },
    "id": "DdvYzWO1tjJq",
    "outputId": "f9e18698-3456-4c4a-8b98-08c037aa35fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average occurrence of words in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "the           13.3888\n",
       "and            6.4880\n",
       "of             5.7838\n",
       "to             5.3343\n",
       "is             4.2572\n",
       "               ...   \n",
       "fume           0.0001\n",
       "fumiko         0.0001\n",
       "quinlan        0.0001\n",
       "quine          0.0001\n",
       "ünfaithful     0.0001\n",
       "Length: 52640, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Average occurrence of words in the dataset:\")\n",
    "dtm.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-B3ytCTQEGK"
   },
   "source": [
    "Notice that in the tables above, there are many vocabulary words that might not be informative for our model (i.e. a lot of zeros in the bag-of-words representation).  We can add a few arguments in order to help make the results more effective. First, let's set the `stop_words` argument to remove common function words in English like 'to', 'the', 'and', etc. We can also restrict the overall number of words in the vocabulary by changing the `max_features` argument. Try out a few different values for `max_features`. How does it change the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1669674690514,
     "user": {
      "displayName": "Emily Grabowski",
      "userId": "07033070354571424802"
     },
     "user_tz": 480
    },
    "id": "jMnye-AW21hw",
    "outputId": "29c3f4d5-dcb6-4e0a-904c-c2ef89ceaf1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the vocabulary: 500\n",
      "Most common words in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "br           4.0964\n",
       "movie        1.7778\n",
       "film         1.5658\n",
       "like         0.8101\n",
       "just         0.7197\n",
       "              ...  \n",
       "enjoyable    0.0343\n",
       "thriller     0.0339\n",
       "events       0.0339\n",
       "actual       0.0339\n",
       "talking      0.0338\n",
       "Length: 500, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2  = CountVectorizer(max_features = 500,stop_words='english') #Load the bag of words model\n",
    "X = vectorizer2.fit_transform(text).toarray() # Make the bag of words representation\n",
    "\n",
    "print(\"Number of items in the vocabulary:\",X.shape[1])\n",
    "term_matrix = pd.DataFrame(X,columns = vectorizer2.get_feature_names_out()) #Make the matrix\n",
    "\n",
    "print(\"Most common words in the dataset:\")\n",
    "term_matrix.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kzHRk8daaaO"
   },
   "source": [
    "## **Demo**: Regression for sentiment analysis\n",
    "\n",
    "Now that we have a numerical representation, we can use our models to pull out patterns in the data. In this case, we will use **logistic regression**, a common type of regression used for binary outcome variables (0=negative, 1=positive). \n",
    "Logistic regression uses an s-curve to calculate te probability of the outcome being 1 or 0.\n",
    "\n",
    "![](https://pimages.toolbox.com/wp-content/uploads/2022/04/11040522/46-4.png)\n",
    "\n",
    "\n",
    "\n",
    "The main intuition of logistic regression is that negative coefficients--> negative sentiment and positive coefficients--> positive sentiment. \n",
    "\n",
    "\n",
    "The code for generating a logistic regression is given below. Are there any surprising results in the most negative and most positive words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1852,
     "status": "ok",
     "timestamp": 1669675089506,
     "user": {
      "displayName": "Emily Grabowski",
      "userId": "07033070354571424802"
     },
     "user_tz": 480
    },
    "id": "4v5KRmUaaYP-",
    "outputId": "847b44fd-8881-4e27-b1b1-bd1597ff7fe1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.330388\n",
      "         Iterations 8\n",
      "Regression coefficients for each word:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "waste       -2.380682\n",
       "awful       -1.708785\n",
       "dull        -1.693389\n",
       "worst       -1.592559\n",
       "horrible    -1.500726\n",
       "               ...   \n",
       "wonderful    1.018785\n",
       "perfect      1.064582\n",
       "amazing      1.070463\n",
       "loved        1.102197\n",
       "excellent    1.361603\n",
       "Length: 500, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "res = sm.Logit(sentiment,term_matrix).fit()\n",
    "print('Regression coefficients for each word:')\n",
    "res.params.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gwchy7U24Rw"
   },
   "source": [
    "We can also predict the sentiment for a new text. Now that we are talking about predictions rather than coefficients, a predicted value closesr to 1 = more positive, and a predicted value close to 0 = more negative. \n",
    "\n",
    "Let's take a couple of examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1669675319556,
     "user": {
      "displayName": "Emily Grabowski",
      "userId": "07033070354571424802"
     },
     "user_tz": 480
    },
    "id": "g02IeNlHqWRR",
    "outputId": "c1ae777d-75bb-4f02-a5c5-da4cd27816d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.007466\n",
       "1    0.874598\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = [\"This is the worst movie so bad, terrible, horrible\",\"This is the best movie ever I'm such a fan, what an excellent performance\"]\n",
    "X = vectorizer2.transform(new_text).toarray() # Make the bag of words representation\n",
    "term_matrix = pd.DataFrame(X,columns = vectorizer2.get_feature_names_out()) #Make the matrix\n",
    "res.predict(term_matrix) #predict with the logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-e2DpZO5duX"
   },
   "source": [
    "**Exercise:** Add at least three new reviews to the `new_text` list. \n",
    "\n",
    "1. Write a review that is predicted to be very positive (near 1), very negative (near 0), and in between (near .5).\n",
    "2. Try to 'trick' the model-- include examples that a human would likely interpret as positive but the model would interpret as negative, or vice versa.  What kind of elements included in the review seem to trick the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsFCx6v86HbY"
   },
   "source": [
    "## Wrapping it up\n",
    "\n",
    "Today we walked through one way to make a numerical representation of text, the **bag-of-words** representation, and used that representation in a logistic model for sentiment analysis. This representation has shortcomings for several reasons:\n",
    "\n",
    "1. Phrases and negation \n",
    "2. Tone/sarcasm \n",
    "3. Frequency bias\n",
    "\n",
    "However, this is still a useful representation because of its simplicity and effectiveness, and can be built upon for other representations. With numeric representations for text, we can now apply many useful statistical tests and models to text data and identify interesting patterns in data. This leads into the field of **computational text analysis**, where there is a lot of really interesting work on identifying patterns in text data.\n",
    "\n",
    "**Final Discussion**: What are some cases where you run into text analysis in your daily life?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMrE6jUS9qDNrFcyi0I4RKC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
