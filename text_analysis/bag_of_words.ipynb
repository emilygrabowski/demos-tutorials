{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrE6jUS9qDNrFcyi0I4RKC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Turning Text into Numbers\n","\n","Learning Objectives:\n","- Introduce bag-of-words representation for text\n","- Apply bag-of-words approach to sentiment analysis task\n","\n","One of the key benefits of data wrangling and analysis is pulling out patterns from complex data. One rich source of data is text, including social media posts to novels to news corpora.\n","\n","The key question surrounding text data is: how do we go from words to numbers that are compatible with models and statistical analysis? Today we will go through an example using movie review data and sentiment analysis for text.\n","\n","\n"],"metadata":{"id":"YFoXOLWRPw_p"}},{"cell_type":"markdown","source":["## Movie Review Dataset and Sentiment Analysis\n","\n","\n","Today we'll use a [data set](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download) which contains reviews of movies from IMDB. These reviews reflect someone's **sentiment**, or attitude, towards the movie they are reviewing, be it positive or negative. \n","\n","Let's load in the dataset and take a look at a few examples.\n"],"metadata":{"id":"wDdQG12Z64AP"}},{"cell_type":"code","source":["!git clone https://github.com/PeterDrake/numbers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ao-P-Pvgd9jC","executionInfo":{"status":"ok","timestamp":1669672572638,"user_tz":480,"elapsed":342,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}},"outputId":"44eed539-0dd4-4cb3-8585-3c9b54df8860"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'numbers' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"n7LWSNC-PslN","executionInfo":{"status":"ok","timestamp":1669672513648,"user_tz":480,"elapsed":154,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}}},"outputs":[],"source":["import csv\n","\n","with open('numbers/IMDB_reviews_subset.csv') as file:\n","  table = list(csv.DictReader(file))\n","text = [row[\"text\"] for row in table]\n","sentiment = [int(row[\"sentiment\"]) for row in table]\n"]},{"cell_type":"markdown","source":["**Question:** Are the reviews below negative or positive? What specific words or phrases from the text helped you to make that judgment?"],"metadata":{"id":"iql7Aal96FEB"}},{"cell_type":"code","source":["print(\"Example 1:\")\n","text[14]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"erWpHxkLjBzp","executionInfo":{"status":"ok","timestamp":1669670656833,"user_tz":480,"elapsed":631,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}},"outputId":"066bfdeb-c0cd-4adf-fc5f-4a299906ea6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example 1:\n"]},{"output_type":"execute_result","data":{"text/plain":["\"This a fantastic movie of three prisoners who become famous. One of the actors is george clooney and I'm not a fan but this roll is not bad. Another good thing about the movie is the soundtrack (The man of constant sorrow). I recommand this movie to everybody. Greetings Bart\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["print(\"Example 2:\")\n","text[19]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"WfKfZ3A-lZaF","executionInfo":{"status":"ok","timestamp":1669670658549,"user_tz":480,"elapsed":238,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}},"outputId":"5603191f-2ef1-4409-8a82-76ea6544d18c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example 2:\n"]},{"output_type":"execute_result","data":{"text/plain":["\"An awful film! It must have been up against some real stinkers to be nominated for the Golden Globe. They've taken the story of the first famous female Renaissance painter and mangled it beyond recognition. My complaint is not that they've taken liberties with the facts; if the story were good, that would perfectly fine. But it's simply bizarre -- by all accounts the true story of this artist would have made for a far better film, so why did they come up with this dishwater-dull script? I suppose there weren't enough naked people in the factual version. It's hurriedly capped off in the end with a summary of the artist's life -- we could have saved ourselves a couple of hours if they'd favored the rest of the film with same brevity.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["print('Example 3:')\n","text[7]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"RBVUIn5hlc0y","executionInfo":{"status":"ok","timestamp":1669670659859,"user_tz":480,"elapsed":11,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}},"outputId":"b00f8e83-1790-43bf-9413-f308032f83d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example 3:\n"]},{"output_type":"execute_result","data":{"text/plain":["\"This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can't believe it's still on the air.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## Bag-of-words\n","\n","\n","Now that we've taken a look at the dataset, let's talk about transforming the text to numbers. In this case, we need a framework in which to assign meaningful numeric values to each review. One way to simplify this problem is to treat the text as a collection of unordered words, which alleviates the need to account for things like word order.\n","\n","For example, in judging the sentiment of each of those reviews, there are certain keywords and phrases that are helpful for identifying sentiment. \n","\n","1. Positive sentiment: Good, happy, best, awesome\n","2. Negative sentiment: Worst, terrible, horrible, bad\n","\n","The idea that individual words are informative as to the content of a text is the logic underpinning the bag-of-words approach. You take the words in a review, put it in a bag, shake it up, and count the occurence of each word in the review. And now we have numbers!\n","\n","![link text](https://dudeperf3ct.github.io/images/lstm_and_gru/bag-of-words.png)\n","\n","\n","[Image source](https://dudeperf3ct.github.io/images/lstm_and_gru/)\n","\n","\n","**Question:** What are some cases where a bag-of-words approach might not work? (i.e. where context changes the meaning of words)"],"metadata":{"id":"dt9j9D1n-KuT"}},{"cell_type":"markdown","source":["## Bag-of-words in Python\n","\n","The steps to actually generating this result is as follows:\n","1. Preprocess the text (lower case, remove punctuation and formatting)\n","2. Identify all of the unique words in the text\n","3. Count the occurrence of each unique word in each review (which can be 0)\n","4. Make a table of reviews x vocabulary \n","\n","\n","This is all possible to do in base Python, but the more efficient and common way is to use a package developed for this purpose which will roll all of these steps together. \n","\n","Today, we will use a function called [CountVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which will *count* up all of the occurences of each word in the text and make a *vector*. We won't get into the details of the whole package that this comes from (sklearn, which is an excellent machine learning package) today, but I'll walk through the example code below:"],"metadata":{"id":"W1U31dfdHOpw"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer  = CountVectorizer() #Load the bag of words model\n","X = vectorizer.fit_transform(text).toarray() # Make the bag of words representation and transform it into a matrix\n","\n","\n","print(\"Number of items in the vocabulary:\",X.shape[1])\n","dtm = pd.DataFrame(X,columns = vectorizer.get_feature_names_out()) #Add vocabulary labels to the matrix\n","dtm\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"id":"f4Hiu04T1TUQ","executionInfo":{"status":"ok","timestamp":1669673654990,"user_tz":480,"elapsed":852,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}},"outputId":"4e8501c3-a748-413b-cdb5-c2aedf304c06"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of items in the vocabulary: 25136\n"]},{"output_type":"execute_result","data":{"text/plain":["      00  000  007  00am  01pm  02  04  06  07  08  ...  zwick  \\\n","0      0    0    0     0     0   0   0   0   0   0  ...      0   \n","1      0    0    0     0     0   0   0   0   0   0  ...      0   \n","2      0    0    0     0     0   0   0   0   0   0  ...      0   \n","3      0    0    0     0     0   0   0   0   0   0  ...      0   \n","4      0    0    0     0     0   0   0   0   0   0  ...      0   \n","...   ..  ...  ...   ...   ...  ..  ..  ..  ..  ..  ...    ...   \n","1995   0    0    0     0     0   0   0   0   0   0  ...      0   \n","1996   0    0    0     0     0   0   0   0   0   0  ...      0   \n","1997   0    0    0     0     0   0   0   0   0   0  ...      0   \n","1998   0    0    0     0     0   0   0   0   0   0  ...      0   \n","1999   0    0    0     0     0   0   0   0   0   0  ...      0   \n","\n","      zzzzzzzzzzzzzzzzzz  álvaro  ángel  æon  élan  être  ís  ísnt  île  \n","0                      0       0      0    0     0     0   0     0    0  \n","1                      0       0      0    0     0     0   0     0    0  \n","2                      0       0      0    0     0     0   0     0    0  \n","3                      0       0      0    0     0     0   0     0    0  \n","4                      0       0      0    0     0     0   0     0    0  \n","...                  ...     ...    ...  ...   ...   ...  ..   ...  ...  \n","1995                   0       0      0    0     0     0   0     0    0  \n","1996                   0       0      0    0     0     0   0     0    0  \n","1997                   0       0      0    0     0     0   0     0    0  \n","1998                   0       0      0    0     0     0   0     0    0  \n","1999                   0       0      0    0     0     0   0     0    0  \n","\n","[2000 rows x 25136 columns]"],"text/html":["\n","  <div id=\"df-a3192c66-ad0f-4f52-90d7-539ac9089aa0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>00</th>\n","      <th>000</th>\n","      <th>007</th>\n","      <th>00am</th>\n","      <th>01pm</th>\n","      <th>02</th>\n","      <th>04</th>\n","      <th>06</th>\n","      <th>07</th>\n","      <th>08</th>\n","      <th>...</th>\n","      <th>zwick</th>\n","      <th>zzzzzzzzzzzzzzzzzz</th>\n","      <th>álvaro</th>\n","      <th>ángel</th>\n","      <th>æon</th>\n","      <th>élan</th>\n","      <th>être</th>\n","      <th>ís</th>\n","      <th>ísnt</th>\n","      <th>île</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows × 25136 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3192c66-ad0f-4f52-90d7-539ac9089aa0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a3192c66-ad0f-4f52-90d7-539ac9089aa0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a3192c66-ad0f-4f52-90d7-539ac9089aa0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Now, let's answer some questions about the table: \n","1. How many reviews are in the table?\n","2. How many unique words are in the table?\n","3. What does each of the numbers stand for? What does a row correspond to?\n","\n","Now let's take a look at the most common words in the data. Again, we won't worry too much about the code and will focus on the output of the cell below:"],"metadata":{"id":"Mw3JlhtnOGX2"}},{"cell_type":"code","source":["print(\"Average occurrence of words in the dataset:\")\n","dtm.mean().sort_values(ascending=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DdvYzWO1tjJq","executionInfo":{"status":"ok","timestamp":1669673935209,"user_tz":480,"elapsed":183,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}},"outputId":"f9e18698-3456-4c4a-8b98-08c037aa35fd"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Average occurrence of words in the dataset:\n"]},{"output_type":"execute_result","data":{"text/plain":["the           13.0870\n","and            6.3200\n","of             5.6650\n","to             5.2210\n","br             4.0490\n","               ...   \n","offsetting     0.0005\n","offshore       0.0005\n","oftentimes     0.0005\n","ogles          0.0005\n","île            0.0005\n","Length: 25136, dtype: float64"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Notice that in the tables above, there are many vocabulary words that might not be informative for our model (i.e. a lot of zeros in the bag-of-words representation).  We can add a few arguments in order to help make the results more effective. First, let's set the `stop_words` argument to remove common function words in English like 'to', 'the', 'and', etc. We can also restrict the overall number of words in the vocabulary by changing the `max_features` argument. Try out a few different values for `max_features`. How does it change the results?\n"],"metadata":{"id":"z-B3ytCTQEGK"}},{"cell_type":"code","source":["vectorizer2  = CountVectorizer(max_features = 500,stop_words='english') #Load the bag of words model\n","X = vectorizer2.fit_transform(text).toarray() # Make the bag of words representation\n","\n","print(\"Number of items in the vocabulary:\",X.shape[1])\n","term_matrix = pd.DataFrame(X,columns = vectorizer2.get_feature_names_out()) #Make the matrix\n","\n","print(\"Most common words in the dataset:\")\n","term_matrix.mean().sort_values(ascending=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMnye-AW21hw","executionInfo":{"status":"ok","timestamp":1669674690514,"user_tz":480,"elapsed":374,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}},"outputId":"29c3f4d5-dcb6-4e0a-904c-c2ef89ceaf1b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of items in the vocabulary: 500\n","Most common words in the dataset:\n"]},{"output_type":"execute_result","data":{"text/plain":["br         4.0490\n","movie      1.7790\n","film       1.5365\n","like       0.7755\n","just       0.7130\n","            ...  \n","crazy      0.0335\n","subject    0.0335\n","career     0.0335\n","sad        0.0335\n","fight      0.0335\n","Length: 500, dtype: float64"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## **Demo**: Regression for sentiment analysis\n","\n","Now that we have a numerical representation, we can use our models to pull out patterns in the data. In this case, we will use **logistic regression**, a common type of regression used for binary outcome variables (0=negative, 1=positive). \n","Logistic regression uses an s-curve to calculate te probability of the outcome being 1 or 0.\n","\n","![](https://pimages.toolbox.com/wp-content/uploads/2022/04/11040522/46-4.png)\n","\n","\n","\n","The main intuition of logistic regression is that negative coefficients--> negative sentiment and positive coefficients--> positive sentiment. \n","\n","\n","The code for generating a logistic regression is given below. Are there any surprising results in the most negative and most positive words?"],"metadata":{"id":"9kzHRk8daaaO"}},{"cell_type":"code","source":["import statsmodels.api as sm\n","\n","res = sm.Logit(sentiment,term_matrix).fit()\n","print('Regression coefficients for each word:')\n","res.params.sort_values()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4v5KRmUaaYP-","executionInfo":{"status":"ok","timestamp":1669675089506,"user_tz":480,"elapsed":1852,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}},"outputId":"847b44fd-8881-4e27-b1b1-bd1597ff7fe1"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimization terminated successfully.\n","         Current function value: 0.153313\n","         Iterations 12\n","Regression coefficients for each word:\n"]},{"output_type":"execute_result","data":{"text/plain":["wouldn      -6.197348\n","obviously   -5.356856\n","entire      -5.339880\n","save        -5.240878\n","waste       -5.196089\n","               ...   \n","fine         6.844167\n","highly       6.905720\n","excellent    6.988131\n","works        7.270865\n","reality      8.911016\n","Length: 500, dtype: float64"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["We can also predict the sentiment for a new text. Now that we are talking about predictions rather than coefficients, a predicted value closesr to 1 = more positive, and a predicted value close to 0 = more negative. \n","\n","Let's take a couple of examples below."],"metadata":{"id":"_gwchy7U24Rw"}},{"cell_type":"code","source":["new_text = [\"This is the worst movie so bad, terrible, horrible\",\"This is the best movie ever I'm such a fan, what an excellent performance\"]\n","X = vectorizer2.transform(new_text).toarray() # Make the bag of words representation\n","term_matrix = pd.DataFrame(X,columns = vectorizer2.get_feature_names_out()) #Make the matrix\n","res.predict(term_matrix) #predict with the logistic regression\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g02IeNlHqWRR","executionInfo":{"status":"ok","timestamp":1669675319556,"user_tz":480,"elapsed":143,"user":{"displayName":"Emily Grabowski","userId":"07033070354571424802"}},"outputId":"c1ae777d-75bb-4f02-a5c5-da4cd27816d0"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    2.273448e-07\n","1    9.999404e-01\n","dtype: float64"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["**Exercise:** Add at least three new reviews to the `new_text` list. \n","\n","1. Write a review that is predicted to be very positive (near 1), very negative (near 0), and in between (near .5).\n","2. Try to 'trick' the model-- include examples that a human would likely interpret as positive but the model would interpret as negative, or vice versa.  What kind of elements included in the review seem to trick the model? "],"metadata":{"id":"Y-e2DpZO5duX"}},{"cell_type":"markdown","source":["## Wrapping it up\n","\n","Today we walked through one way to make a numerical representation of text, the **bag-of-words** representation, and used that representation in a logistic model for sentiment analysis. This representation has shortcomings for several reasons:\n","\n","1. Phrases and negation \n","2. Tone/sarcasm \n","3. Frequency bias\n","\n","However, this is still a useful representation because of its simplicity and effectiveness, and can be built upon for other representations. With numeric representations for text, we can now apply many useful statistical tests and models to text data and identify interesting patterns in data. This leads into the field of **computational text analysis**, where there is a lot of really interesting work on identifying patterns in text data.\n","\n","**Final Discussion**: What are some cases where you run into text analysis in your daily life?"],"metadata":{"id":"vsFCx6v86HbY"}}]}